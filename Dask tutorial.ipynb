{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2da5320",
   "metadata": {},
   "source": [
    "# Dask tutorial\n",
    "\n",
    "## Dask delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b6d7d",
   "metadata": {},
   "source": [
    "### Basics \n",
    "\n",
    "Sometimes we face problems that are parallelizable, but don’t fit into high-level abstractions like Dask Array or Dask DataFrame. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae9d344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9268580f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    return x * 2\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = inc(x)\n",
    "    b = double(x)\n",
    "    c = add(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = sum(output)\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a9b5c",
   "metadata": {},
   "source": [
    "There is clearly **parallelism** in this problem - many of the inc, double, and add functions can evaluate independently, but it’s not clear how to convert this to a big array or big DataFrame computation.\n",
    "\n",
    "As written, this code runs sequentially in a single thread. However,  a lot of this could be executed in parallel.\n",
    "\n",
    "The Dask delayed function decorates  functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "617d2a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('sum-0dca6a9e-429a-4dbd-88f2-3c239e5e6831')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = dask.delayed(inc)(x)\n",
    "    b = dask.delayed(double)(x)\n",
    "    c = dask.delayed(add)(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = dask.delayed(sum)(output)\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4ae11",
   "metadata": {},
   "source": [
    "We used the `dask.delayed` function to wrap the function calls that we want to turn into tasks. None of the inc, double, add, or sum calls have happened yet. Instead, the object total is a Delayed result that contains a task graph of the entire computation. Looking at the graph we see clear opportunities for parallel execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404086d9",
   "metadata": {},
   "source": [
    "`total.visualize()`\n",
    "to visualizate graph you should have Graphviz. I have some comlication with instalation. So you can just uncomment following line or take a look at the graph in the presentation. Sorry for inconvinience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55716b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d19b727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872430c",
   "metadata": {},
   "source": [
    "### Decorator\n",
    "\n",
    "Delayed function can be used as a decorator. Here is a reproduction of our original problem as a parallel code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fe77f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "@dask.delayed\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "@dask.delayed\n",
    "def double(x):\n",
    "    return x * 2\n",
    "\n",
    "@dask.delayed\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = inc(x)\n",
    "    b = double(x)\n",
    "    c = add(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = dask.delayed(sum)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "422febcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00517488",
   "metadata": {},
   "source": [
    "### Parallelize a for loop\n",
    "\n",
    "`for loops` are one of the most common things that we want to parallelize. Use `dask.delayed` on inc and sum to parallelize the computation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cd317a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51c52be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Sequential code\n",
    "\n",
    "results = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    results.append(y)\n",
    "    \n",
    "total = sum(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7711aa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('add-858b337652043b59a3d54832a961cf97')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f116003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "\n",
    "for x in data:\n",
    "    y = delayed(inc)(x)\n",
    "    results.append(y)\n",
    "    \n",
    "total = delayed(sum)(results)\n",
    "\n",
    "total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5551f",
   "metadata": {},
   "source": [
    "More relevant examples you can find by following links:\n",
    "* https://github.com/dask/dask-tutorial/blob/main/01_dask.delayed.ipynb\n",
    "* https://docs.dask.org/en/latest/delayed.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5276f",
   "metadata": {},
   "source": [
    "## Dask future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd31a2",
   "metadata": {},
   "source": [
    "Dask futures provide fine-grained real-time execution for custom situations. This is the foundation for other APIs like Dask arrays and dataframes.\n",
    "Unlike for arrays and dataframes, you need the Dask client to use the Futures interface.\n",
    "This interface is good for task scheduling like `dask.delayed`, but is immediate rather than lazy, which provides some more flexibility in situations where the computations may evolve over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52283a4c",
   "metadata": {},
   "source": [
    "### Start Dask Client\n",
    "\n",
    "First of all, we need start a Client to use the futures interface. This tracks state among the various worker processes or threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a8326af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\distributed\\node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 59059 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-4433c32b-d6a0-11ec-80f4-94659cc66d4d</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:59059/status\" target=\"_blank\">http://127.0.0.1:59059/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">1c02165f</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:59059/status\" target=\"_blank\">http://127.0.0.1:59059/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 1\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 4\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 3.90 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-73103f99-d355-4db0-9696-665cd3dd0cf4</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:59060\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 1\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:59059/status\" target=\"_blank\">http://127.0.0.1:59059/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 4\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 3.90 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:59071\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 4\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:59072/status\" target=\"_blank\">http://127.0.0.1:59072/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 3.90 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:59063\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> C:\\Users\\Дом\\Desktop\\jupyter\\dask-worker-space\\worker-wc__osv7\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:59060' processes=1 threads=4, memory=3.90 GiB>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(threads_per_worker=4, n_workers=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433270d",
   "metadata": {},
   "source": [
    "`Client.submit(func, *args[, key, workers, ...])` - Submit a function application to the scheduler\n",
    "\n",
    "`Client.map(func, *iterables[, key, workers, ...])` - Map a function on a sequence of arguments\n",
    "\n",
    "`Future.result([timeout])` - Wait until computation completes, gather result to local process\n",
    "\n",
    "*...*\n",
    "\n",
    "More information about Client you can find by this link https://docs.dask.org/en/latest/futures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed0a71",
   "metadata": {},
   "source": [
    "### Create simple functions\n",
    "\n",
    "These functions do simple operations like add two numbers together, but they sleep for a random amount of time to simulate real work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e71205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(random.random())\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    time.sleep(random.random())\n",
    "    return 2 * x\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(random.random())\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaccd0e",
   "metadata": {},
   "source": [
    "We can run them locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6ecc8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981f29e",
   "metadata": {},
   "source": [
    "Or we can submit them to run remotely with Dask. This immediately returns a future that points to the ongoing computation, and eventually to the stored result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c98dfcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>Future: inc</strong>\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> status: </span>\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-error-color0, black)\">pending</span>,\n",
       "\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> type:</span> NoneType,\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> key:</span> inc-4ff8911865c58d2d52090822cf1fd98c"
      ],
      "text/plain": [
       "<Future: pending, key: inc-4ff8911865c58d2d52090822cf1fd98c>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future = client.submit(inc, 1)  # returns immediately with pending future\n",
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee190163",
   "metadata": {},
   "source": [
    "If you wait a second, and then check on the future again, you’ll see that it has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08ea1b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>Future: inc</strong>\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> status: </span>\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-error-color0, black)\">finished</span>,\n",
       "\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> type:</span> int,\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> key:</span> inc-4ff8911865c58d2d52090822cf1fd98c"
      ],
      "text/plain": [
       "<Future: finished, type: int, key: inc-4ff8911865c58d2d52090822cf1fd98c>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future  # scheduler and client talk constantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c703d",
   "metadata": {},
   "source": [
    "You can block on the computation and gather the result with the .result() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bad0207f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f75062",
   "metadata": {},
   "source": [
    "### Chain dependencies\n",
    "\n",
    "You can submit tasks on other futures. This will create a dependency between the inputs and outputs. Dask will track the execution of all tasks, ensuring that downstream tasks are run at the proper time and place and with the proper data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04392c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>Future: add</strong>\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> status: </span>\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-error-color0, black)\">pending</span>,\n",
       "\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> type:</span> NoneType,\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> key:</span> add-2677dd63462575e162e9c9c98b7347a0"
      ],
      "text/plain": [
       "<Future: pending, key: add-2677dd63462575e162e9c9c98b7347a0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = client.submit(inc, 1)\n",
    "y = client.submit(double, 2)\n",
    "z = client.submit(add, x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "959e9901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf77f47",
   "metadata": {},
   "source": [
    "Note that we never blocked on x or y nor did we ever have to move their data back to our notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11369ea7",
   "metadata": {},
   "source": [
    "### Submit many tasks\n",
    "\n",
    "So we’ve learned how to run Python functions remotely. This becomes useful when we add two things:\n",
    "* We can submit thousands of tasks per second\n",
    "* Tasks can depend on each other by consuming futures as inputs\n",
    "\n",
    "We submit many tasks that depend on each other in a normal Python for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11af4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f247e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(256):\n",
    "    x = client.submit(inc, i)     # x = inc(i)\n",
    "    y = client.submit(double, x)  # y = inc(x)\n",
    "    z = client.submit(add, x, y)  # z = inc(y)\n",
    "    zs.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b709497",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = client.submit(sum, zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb80f6",
   "metadata": {},
   "source": [
    "To make this go faster, add an additional workers with more cores - this is more practical when using an actual cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de786b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.scale(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe33cde",
   "metadata": {},
   "source": [
    "### Custom computation: Tree summation\n",
    "\n",
    "As an example of a non-trivial algorithm, consider the classic tree reduction. We accomplish this with a nested for loop and a bit of normal Python logic."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b03cba4",
   "metadata": {},
   "source": [
    "finish           total             single output\n",
    "    ^          /        \\\n",
    "    |        c1          c2        neighbors merge\n",
    "    |       /  \\        /  \\\n",
    "    |     b1    b2    b3    b4     neighbors merge\n",
    "    ^    / \\   / \\   / \\   / \\\n",
    "start   a1 a2 a3 a4 a5 a6 a7 a8    many inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9923f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = zs\n",
    "while len(L) > 1:\n",
    "    new_L = []\n",
    "    for i in range(0, len(L), 2):\n",
    "        future = client.submit(add, L[i], L[i + 1])  # add neighbors\n",
    "        new_L.append(future)\n",
    "    L = new_L                                   # swap old list for new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82634c22",
   "metadata": {},
   "source": [
    "If we watch the dashboard’s status page (http://localhost:8787/status), we can note two things:\n",
    "\n",
    "* The red bars are for inter-worker communication. They happen as different workers need to combine their intermediate values\n",
    "\n",
    "* There is lots of parallelism at the beginning but less towards the end as we reach the top of the tree where there is less work to do.\n",
    "\n",
    "More informations about this example you can find by this link https://examples.dask.org/futures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d2355",
   "metadata": {},
   "source": [
    "## Dask Bag\n",
    "\n",
    "Dask Bag implements operations like `map`, `filter`, `fold`, and `groupby` on collections of generic Python objects. It does this in parallel with a small memory footprint using Python iterators. It is similar to a parallel version of PyToolz or a Pythonic version of the PySpark RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd97852",
   "metadata": {},
   "source": [
    "**Common Uses**\n",
    "\n",
    "Dask bags are often used to parallelize simple computations on unstructured or semi-structured data like text data, log files, JSON records, or user defined Python objects.\n",
    "\n",
    "**Execution**\n",
    "Execution on bags provide two benefits:\n",
    "\n",
    "* Parallel: data is split up, allowing multiple cores or machines to execute in parallel\n",
    "\n",
    "* Iterating: data processes lazily, allowing smooth execution of larger-than-memory data, even on a single machine within a single partition\n",
    "\n",
    "**Known Limitations**\n",
    "Bags provide very general computation (any Python function). This generality comes at cost. Bags have the following known limitations:\n",
    "\n",
    "* By default, they rely on the multiprocessing scheduler, which has its own set of known limitations (see Shared Memory)\n",
    "\n",
    "* Bags are immutable and so you can not change individual elements\n",
    "\n",
    "* Bag operations tend to be slower than array/DataFrame computations in the same way that standard Python containers tend to be slower than NumPy arrays and Pandas DataFrames\n",
    "\n",
    "* Bag’s `groupby` is slow. You should try to use Bag’s `foldby` if possible. Using foldby requires more thought though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e8ced",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "85eddbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81e517",
   "metadata": {},
   "source": [
    "The usage of `dask.bag` API involves a list of steps which are commonly used to perform complicated computations in parallel which are as below:\n",
    "\n",
    "* Create dask bag lazy object using `from_sequence()`, `from_delayed()` or `from_url()`.\n",
    "\n",
    "Example: bag = dask.bag.from_sequence(range(1000000000))\n",
    "* Perform list of operations like `map()`, `filter()`, `groupby()`, etc one by one on lazy dask bag object created from step 1.\n",
    "\n",
    "Example: bag_final = bag.map(lambda x : x*x).filter(lambda x: x%2)\n",
    "* Call `compute()` method on final bag object from step 2 which was created after calling all operations.\n",
    "\n",
    "Example: bag_final.compute()\n",
    "\n",
    "Now let's look at these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8be34",
   "metadata": {},
   "source": [
    "### Step 1: Create Lazy bag Objects \n",
    "\n",
    "The `from_sequence()` method is commonly used as a starting point in converting a list of operations into dask compatible operations so that they run in parallel. The `from_sequence()` method accepts a list of values, iterators and converts it to a lazy dask bag object consisting of a list of values which will go input to the next methods called on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3638ca79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<from_sequence, npartitions=100>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag1 = db.from_sequence(range(1000))\n",
    "bag1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128ce38",
   "metadata": {},
   "source": [
    "By default `from_sequence()` will divide data into 100 partitions. We can also explicitly pass a number of values to keep in each partition as well as partition size by setting npartitions and partition_size parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b604de03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<from_sequence, npartitions=1000>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag2 = db.from_sequence(range(1000000), partition_size=1000, npartitions=1000)\n",
    "bag2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca2d48",
   "metadata": {},
   "source": [
    "We'll check size of bag object using sys.getsizeof() which returns the size of the object in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2cc22275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(bag1), sys.getsizeof(bag2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6bee4",
   "metadata": {},
   "source": [
    "We can see that both have a size of 48 bytes even though input to both are different size lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa685f3c",
   "metadata": {},
   "source": [
    "### Step 2: Perform List of Operations on Lazy Bag Object from Step 1\n",
    "\n",
    "We'll now apply a list of commonly available functions to perform various computations on the list of values. These methods will also generate another lazy dask bag object.\n",
    "\n",
    "Below are list of commonly used operations:\n",
    "\n",
    "* `bag_object.map(function)`: It'll apply function passed to `map` on all individual entry of bag_object.\n",
    "* `bag_object.filter(condition)`: It'll check condition passed to `filter` on all individual entry of bag_object and only keep entries which satisfies condition.\n",
    "* `bag_object.product(another_bag)`: It calculates cross product of both bags and creates another bag of that values.\n",
    "* `bag_object.max()`: It returns maximum from list.\n",
    "* `bag_object.min()`: It returns minimum from list.\n",
    "* `bag_object.accumulate()`: It takes as input binary function which operates on two input values and returns one value. This value is given as input as first parameter in next iteration.\n",
    "* `bag_object.count()`: It returns number of values in a bag object.\n",
    "* `bag_object.sum()`: It returns sum of all values of list.\n",
    "* `bag_object.std()`: It returns standard deviation.\n",
    "* `bag_object.frequencies()`: - It returns frequency of each value in bag.\n",
    "* `bag_object.groupby()`: - It groups all values in list based on some key specified. We can then perform operations on these grouped values.\n",
    "* `bag_object.join()`: - It joins one list with another list based on key specified. It merges values where key matches.\n",
    "* `bag_object.topk()`: - It joins one list with another list based on key specified. It merges values where key matches.\n",
    "\n",
    "Please make a note that the above steps will also create another lazy bag object only. It'll only perform actual computation when we call `compute()` on the final bag object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "353ddce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<lambda, npartitions=100>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag1 = bag1.map(lambda x: x*2)\n",
    "\n",
    "final_bag1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26e0d0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<filter-lambda, npartitions=1000>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag2 = bag2.filter(lambda x: x%100 == 0)\n",
    "\n",
    "final_bag2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91d0c3",
   "metadata": {},
   "source": [
    "### Step 3: Call `compute()` on Final Bag Object to Perform Computation in Parallel \n",
    "\n",
    "The final step to actually perform computation in parallel and return result is to call `compute()` method on the final bag object. We'll call compute() on both of our final objects created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8152f341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_bag1.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64af14d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100, 200, 300, 400, 500, 600, 700, 800, 900]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag2.compute()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b2bc8",
   "metadata": {},
   "source": [
    "We can evaluate bag objects from step 1 and it'll return the actual list. We can even directly call the method list passing bag object and it'll also return all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b86499e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size : 8856 bytes\n",
      "Length of Values :  1000\n"
     ]
    }
   ],
   "source": [
    "final_list = bag1.compute()\n",
    "\n",
    "print(\"Size : %d bytes\"%sys.getsizeof(final_list))\n",
    "print(\"Length of Values : \", len(final_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83390e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bag1)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf98201",
   "metadata": {},
   "source": [
    "### Performing All Steps Together\n",
    "\n",
    "`map()` & `filter()` Usage\n",
    "\n",
    "Below created one simple example which loops through 10Mn numbers takes the square of each and only keeps number which is divisible by 100. We have first implemented it with a loop in pure python and then converted it to the dask version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6167e819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100, 200, 300, 400, 500, 600, 700, 800, 900]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list = []\n",
    "\n",
    "for i in range(1000000):\n",
    "    x = i*2\n",
    "    if x%100 == 0:\n",
    "        final_list.append(x)\n",
    "\n",
    "final_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41f19461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100, 200, 300, 400, 500, 600, 700, 800, 900]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag1 = db.from_sequence(range(1000000))\n",
    "\n",
    "result = bag1.map(lambda x: x*2).filter(lambda x : x%100 == 0)\n",
    "\n",
    "result.compute()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20ad9b",
   "metadata": {},
   "source": [
    "`join()` usage\n",
    "\n",
    "example of usage of `join()` function of dask.bag API where we have lists of tuples and we want to sum up values in both lists where first values of tuple matches. We have implemented both pure Python and dask.bag API versions for an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a65b58be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 250), ('b', 450), ('c', 650), ('d', 850), ('e', 1050)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [(\"a\",100), (\"b\",200), (\"c\",300), (\"d\",400), (\"e\",500)]\n",
    "y = [(\"a\",150), (\"b\",250), (\"c\",350), (\"d\",450), (\"e\",550)]\n",
    "\n",
    "result = {}\n",
    "for key, val in x+y:\n",
    "    if key in result:\n",
    "        result[key] += val\n",
    "    else:\n",
    "        result[key] = val\n",
    "\n",
    "list(result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "578188ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 250), ('b', 450), ('c', 650), ('d', 850), ('e', 1050)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag1 = db.from_sequence(x)\n",
    "\n",
    "bag1.join(y, lambda x: x[0]).map(lambda x: (x[0][0], sum([val[1] for val in x]))).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdef2cb",
   "metadata": {},
   "source": [
    "`groupby()` usage \n",
    "\n",
    "example of usage of `groupby()` where we are looping through a list of two values tuple. We want to take a sum of the second value in tuples where the first value is the same. We have explained it below with both normal python and dask.bag API version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "da7316e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 300), ('b', 200), ('c', 300), ('d', 400), ('e', 800)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [(\"a\",100), (\"b\",200), (\"c\",300), (\"d\",400), (\"e\",500), (\"a\",200), (\"e\",300)]\n",
    "\n",
    "result = {}\n",
    "\n",
    "for key, val in x:\n",
    "    if key in result:\n",
    "        result[key] += val\n",
    "    else:\n",
    "        result[key] = val\n",
    "\n",
    "list(result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f867e814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 400), ('e', 800), ('c', 300), ('a', 300), ('b', 200)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag1 = db.from_sequence([(\"a\",100), (\"b\",200), (\"c\",300), (\"d\",400), (\"e\",500), (\"a\",200), (\"e\",300)])\n",
    "\n",
    "bag1.groupby(lambda x: x[0]).map(lambda x: (x[0], sum([i[1] for i in x[1]]))).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea0689",
   "metadata": {},
   "source": [
    "More examples are available by following links:\n",
    "\n",
    "* https://coderzcolumn.com/tutorials/python/dask-bag-parallel-computing-in-python\n",
    "* https://docs.dask.org/en/latest/bag.html\n",
    "* https://github.com/dask/dask-tutorial/blob/main/02_bag.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfeee54",
   "metadata": {},
   "source": [
    "### Saving Output to a File \n",
    "\n",
    "It's advisable to save the output of the bag to save it to another file after computation is complete. It might happen that output after performing `compute()` is very big and can not be held in memory than its better to save it to disk and then verify results.\n",
    "\n",
    "Dask bag provides a list of methods for converting the output to another format and saving it to various file formats. Below are three methods available for converting a bag of values to another format and saving it.\n",
    "\n",
    "* `to_avro()` - It can be used to save a bag of values as Avro files\n",
    "* `to_dataframe()` - It can be used to convert bag of values to dask data frames which is available through the dask.dataframe module and lets us work on pandas data frames in parallel\n",
    "* `to_textfiles()` - It can be used to save a bag of values as text files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080e2b4",
   "metadata": {},
   "source": [
    "## Dask Array \n",
    "\n",
    "Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores. We coordinate these blocked algorithms using Dask graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651cbe57",
   "metadata": {},
   "source": [
    "### Scope\n",
    "\n",
    "Dask arrays support most of the NumPy interface like the following:\n",
    "\n",
    "* Arithmetic and scalar mathematics: `+`, `*`, `exp`, `log, ...\n",
    "* Reductions along axes: `sum()`, `mean()`, `std()`, `sum(axis=0)`, ...\n",
    "* Tensor contractions / dot products / matrix multiply: `tensordot`\n",
    "* Axis reordering / transpose: `transpose`\n",
    "* Slicing: `x[:100, 500:100:-2]`\n",
    "* Fancy indexing along single axes with lists or NumPy arrays: `x[:, [10, 1, 5]]`\n",
    "* Array protocols like `__array__` and `__array_ufunc__`\n",
    "* Some linear algebra: `svd`, `qr`, `solve`, `solve_triangular`, `lstsq`\n",
    "* …\n",
    "\n",
    "**However**, Dask Array does not implement the entire NumPy interface. Users expecting this will be disappointed. Notably, Dask Array lacks the following features:\n",
    "\n",
    "* Much of `np.linalg` has not been implemented. This has been done by a number of excellent BLAS/LAPACK implementations, and is the focus of numerous ongoing academic research projects\n",
    "* Arrays with unknown shapes do not support all operations\n",
    "* Operations like `sort` which are notoriously difficult to do in parallel, and are of somewhat diminished value on very large data (you rarely actually need a full sort). Often we include parallel-friendly alternatives like `topk`\n",
    "* Dask Array doesn’t implement operations like `tolist` that would be very inefficient for larger datasets. Likewise, it is very inefficient to iterate over a Dask array with for loops\n",
    "* Dask development is driven by immediate need, hence many lesser used functions have not been implemented. Community contributions are encouraged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f24a7e",
   "metadata": {},
   "source": [
    "This creates a `10000x10000` array of random numbers, represented as many numpy arrays of size 1000x1000 (or smaller if the array cannot be divided evenly). In this case there are 100 (10x10) numpy arrays of size 1000x1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24ac150f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 762.94 MiB </td>\n",
       "                        <td> 7.63 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (10000, 10000) </td>\n",
       "                        <td> (1000, 1000) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 100 Tasks </td>\n",
       "                        <td> 100 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"170\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"120\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"120\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"120\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"48\" x2=\"120\" y2=\"48\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"120\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"120\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"84\" x2=\"120\" y2=\"84\" />\n",
       "  <line x1=\"0\" y1=\"96\" x2=\"120\" y2=\"96\" />\n",
       "  <line x1=\"0\" y1=\"108\" x2=\"120\" y2=\"108\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"120\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"120\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"120\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"120\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"120\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"120\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"120\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"120\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"120\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >10000</text>\n",
       "  <text x=\"140.000000\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,60.000000)\">10000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000, 10000), dtype=float64, chunksize=(1000, 1000), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71b17b",
   "metadata": {},
   "source": [
    "Use NumPy syntax as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f2a30f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 39.06 kiB </td>\n",
       "                        <td> 3.91 kiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (5000,) </td>\n",
       "                        <td> (500,) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 430 Tasks </td>\n",
       "                        <td> 10 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"25\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"25\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"25\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"25\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"25\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"25\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"25\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >5000</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<mean_agg-aggregate, shape=(5000,), dtype=float64, chunksize=(500,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e3cc5",
   "metadata": {},
   "source": [
    "Call `.compute()` when you want your result as a NumPy array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6419f",
   "metadata": {},
   "source": [
    "## Persist data in memory\n",
    "\n",
    "If you have the available RAM for your dataset then you can persist data in memory.\n",
    "This allows future computations to be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "01339a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "228dd951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9515086969580655"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time y[0, 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8908e754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99991960.07231274"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908d7fa",
   "metadata": {},
   "source": [
    "The detailed examples about Dask Array could be found by this link https://github.com/dask/dask-tutorial/blob/main/03_array.ipynb\n",
    "\n",
    "**It also includes Blocked Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ef84f",
   "metadata": {},
   "source": [
    "## Dask Dataframe\n",
    "\n",
    "Dask `DataFrame` is a large parallel `DataFrame` composed of many smaller Pandas `DataFrames`, split along the index. These Pandas `DataFrames` may live on disk for larger-than-memory computing on a single machine, or on many different machines in a cluster. One Dask `DataFrame` operation triggers many operations on the constituent Pandas `DataFrames`.\n",
    "\n",
    "**Main Take-aways**\n",
    "\n",
    "* Dask DataFrame should be familiar to Pandas users\n",
    "* The partitioning of dataframes is important for efficient execution\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "Dask.dataframe only covers a small but well-used portion of the Pandas API. This limitation is for two reasons:\n",
    "\n",
    "* The Pandas API is huge\n",
    "* Some operations are genuinely hard to do in parallel (e.g. `sort`)\n",
    "\n",
    "Additionally, some important operations like `set_index work`, but are slower than in Pandas because they include substantial shuffling of data, and may write out to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a89af",
   "metadata": {},
   "source": [
    "### Download the NYC Flights dataset \n",
    "\n",
    "Load data with an extract of flights in the USA across several years. This data is specific to flights out of the three airports in the New York City area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c51084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Downloading NYC Flights dataset... Done!\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "print(\"- Downloading NYC Flights dataset... \", end='', flush=True)\n",
    "url = \"https://storage.googleapis.com/dask-tutorial-data/nycflights.tar.gz\"\n",
    "filename, headers = urllib.request.urlretrieve(url, 'nycflights.tar.gz')\n",
    "print(\"Done!\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26cbd649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# extract the .csv files from the tar file\n",
    "with tarfile.open(filename, mode='r:gz') as flights:\n",
    "            flights.extractall('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c18b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>CRSElapsedTime</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 10 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                          Date DayOfWeek  DepTime CRSDepTime  ArrTime CRSArrTime UniqueCarrier FlightNum  TailNum ActualElapsedTime CRSElapsedTime  AirTime ArrDelay DepDelay  Origin    Dest Distance   TaxiIn  TaxiOut Cancelled Diverted\n",
       "npartitions=10                                                                                                                                                                                                                             \n",
       "                datetime64[ns]     int64  float64      int64  float64      int64        object     int64  float64           float64          int64  float64  float64  float64  object  object  float64  float64  float64     int64    int64\n",
       "                           ...       ...      ...        ...      ...        ...           ...       ...      ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "...                        ...       ...      ...        ...      ...        ...           ...       ...      ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "                           ...       ...      ...        ...      ...        ...           ...       ...      ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "                           ...       ...      ...        ...      ...        ...           ...       ...      ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "Dask Name: read-csv, 10 tasks"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9cd2de",
   "metadata": {},
   "source": [
    "Notice that the respresentation of the dataframe object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1ce231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1621.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1747.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-01-03</td>\n",
       "      <td>3</td>\n",
       "      <td>1546.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-01-04</td>\n",
       "      <td>4</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>1549.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1706.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
       "0 1990-01-01          1   1621.0        1540   1747.0        1701   \n",
       "1 1990-01-02          2   1547.0        1540   1700.0        1701   \n",
       "2 1990-01-03          3   1546.0        1540   1710.0        1701   \n",
       "3 1990-01-04          4   1542.0        1540   1710.0        1701   \n",
       "4 1990-01-05          5   1549.0        1540   1706.0        1701   \n",
       "\n",
       "  UniqueCarrier  FlightNum  TailNum  ActualElapsedTime  ...  AirTime  \\\n",
       "0            US         33      NaN               86.0  ...      NaN   \n",
       "1            US         33      NaN               73.0  ...      NaN   \n",
       "2            US         33      NaN               84.0  ...      NaN   \n",
       "3            US         33      NaN               88.0  ...      NaN   \n",
       "4            US         33      NaN               77.0  ...      NaN   \n",
       "\n",
       "   ArrDelay  DepDelay  Origin Dest Distance  TaxiIn  TaxiOut  Cancelled  \\\n",
       "0      46.0      41.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "1      -1.0       7.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "2       9.0       6.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "3       9.0       2.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "4       5.0       9.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "\n",
       "   Diverted  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd056f",
   "metadata": {},
   "source": [
    "`df.tail()` this command will give an error\n",
    "**In order to prevent this error** we should infer the correct type for coloumns with mixed dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d89a8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269176</th>\n",
       "      <td>1999-12-27</td>\n",
       "      <td>1</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N516UA</td>\n",
       "      <td>225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269177</th>\n",
       "      <td>1999-12-28</td>\n",
       "      <td>2</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N504UA</td>\n",
       "      <td>242.0</td>\n",
       "      <td>...</td>\n",
       "      <td>214.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269178</th>\n",
       "      <td>1999-12-29</td>\n",
       "      <td>3</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1846.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N592UA</td>\n",
       "      <td>240.0</td>\n",
       "      <td>...</td>\n",
       "      <td>220.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269179</th>\n",
       "      <td>1999-12-30</td>\n",
       "      <td>4</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1908.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N575UA</td>\n",
       "      <td>257.0</td>\n",
       "      <td>...</td>\n",
       "      <td>233.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269180</th>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N539UA</td>\n",
       "      <td>249.0</td>\n",
       "      <td>...</td>\n",
       "      <td>232.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
       "269176 1999-12-27          1   1645.0        1645   1830.0        1901   \n",
       "269177 1999-12-28          2   1726.0        1645   1928.0        1901   \n",
       "269178 1999-12-29          3   1646.0        1645   1846.0        1901   \n",
       "269179 1999-12-30          4   1651.0        1645   1908.0        1901   \n",
       "269180 1999-12-31          5   1642.0        1645   1851.0        1901   \n",
       "\n",
       "       UniqueCarrier  FlightNum TailNum  ActualElapsedTime  ...  AirTime  \\\n",
       "269176            UA       1753  N516UA              225.0  ...    205.0   \n",
       "269177            UA       1753  N504UA              242.0  ...    214.0   \n",
       "269178            UA       1753  N592UA              240.0  ...    220.0   \n",
       "269179            UA       1753  N575UA              257.0  ...    233.0   \n",
       "269180            UA       1753  N539UA              249.0  ...    232.0   \n",
       "\n",
       "        ArrDelay  DepDelay  Origin Dest Distance  TaxiIn  TaxiOut  Cancelled  \\\n",
       "269176     -31.0       0.0     LGA  DEN   1619.0     7.0     13.0      False   \n",
       "269177      27.0      41.0     LGA  DEN   1619.0     5.0     23.0      False   \n",
       "269178     -15.0       1.0     LGA  DEN   1619.0     5.0     15.0      False   \n",
       "269179       7.0       6.0     LGA  DEN   1619.0     5.0     19.0      False   \n",
       "269180     -10.0      -3.0     LGA  DEN   1619.0     6.0     11.0      False   \n",
       "\n",
       "        Diverted  \n",
       "269176         0  \n",
       "269177         0  \n",
       "269178         0  \n",
       "269179         0  \n",
       "269180         0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fc5ef",
   "metadata": {},
   "source": [
    "### Basic examples\n",
    "\n",
    "Let's find out following things with Dask DataFrames\n",
    "\n",
    "1. In total, how many non-cancelled flights were taken from each airport?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "505ad348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Origin\n",
       "EWR    1139451\n",
       "JFK     427243\n",
       "LGA     974267\n",
       "Name: Origin, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0eb336",
   "metadata": {},
   "source": [
    "2.  What was the average departure delay from each airport?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c3ee78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Origin\n",
       "EWR    10.295469\n",
       "JFK    10.351299\n",
       "LGA     7.431142\n",
       "Name: DepDelay, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Origin\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb0c785",
   "metadata": {},
   "source": [
    "3. What day of the week has the worst average departure delay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a0aedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DayOfWeek\n",
       "1     8.096565\n",
       "2     8.149109\n",
       "3     9.141912\n",
       "4    10.538275\n",
       "5    11.476687\n",
       "6     7.824071\n",
       "7     8.994296\n",
       "Name: DepDelay, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"DayOfWeek\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567b8b9",
   "metadata": {},
   "source": [
    "### Sharing Intermediate Results\n",
    "\n",
    "\n",
    "When computing all of the above, we sometimes did the same operation more than once. For most operations, dask.dataframe hashes the arguments, allowing duplicate computations to be shared, and only computed once.\n",
    "\n",
    "For example, lets compute the mean and standard deviation for departure delay of all non-canceled flights. Since dask operations are lazy, those values aren't the final results yet. They're just the recipe required to get the result.\n",
    "\n",
    "If we compute them with two calls to compute, there is no sharing of intermediate computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3663616",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = df[~df.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bcd7659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe1f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e128b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee22fbc",
   "metadata": {},
   "source": [
    "Using `dask.compute` takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling dask.compute, allowing shared operations to only be done once instead of twice. In particular, using dask.compute only does the following once:\n",
    "\n",
    "* the calls to `read_csv`\n",
    "* the filter `(df[~df.Cancelled])`\n",
    "* some of the necessary reductions (`sum`, `count`)\n",
    "\n",
    "More examples of Dask DataFrame can be find by following links:\n",
    "\n",
    "* https://github.com/dask/dask-tutorial/blob/main/04_dataframe.ipynb\n",
    "* https://nbviewer.org/github/danbochman/Open-Source-Spotlight/blob/master/Dask/Dask.ipynb\n",
    "* https://examples.dask.org/dataframe.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4a255",
   "metadata": {},
   "source": [
    "## Dask ML \n",
    "\n",
    "Dask-ML provides scalable machine learning in Python using Dask alongside popular machine learning libraries like Scikit-Learn, XGBoost, and others.\n",
    "\n",
    "There are a couple of distinct scaling problems you might face. The scaling strategy depends on which problem you're facing\n",
    "\n",
    "* For in-memory problems, just use scikit-learn (or your favorite ML library)\n",
    "* For large models, use `dask_ml.joblib` and your favorite scikit-learn estimator\n",
    "* For large datasets, use `dask_ml` estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6fbfb3",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "`dask_ml.preprocessing` contains some scikit-learn style transformers that can be used in Pipelines to perform various data transformations as part of the model fitting process. These transformers will work well on dask collections (`dask.array`, `dask.dataframe`), NumPy arrays, or pandas dataframes. They’ll fit and transform in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c66c6cd",
   "metadata": {},
   "source": [
    "###  Scikit-Learn Clones\n",
    "\n",
    "Some of the transformers are (mostly) drop-in replacements for their scikit-learn counterparts.\n",
    "\n",
    "* `MinMaxScaler([feature_range, copy, clip])` - Transform features by scaling each feature to a given range.\n",
    "\n",
    "* `QuantileTransformer(*[, n_quantiles, ...])` - Transforms features using quantile information.\n",
    "\n",
    "* `RobustScaler(*[, with_centering, ...])` - Scale features using statistics that are robust to outliers.\n",
    "\n",
    "* `StandardScaler(*[, copy, with_mean, with_std])` - Standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "* `LabelEncoder([use_categorical])` - Encode labels with value between 0 and n_classes-1.\n",
    "\n",
    "* `OneHotEncoder(n_values, ...)` - Encode categorical integer features as a one-hot numeric array.\n",
    "\n",
    "* `PolynomialFeatures([degree, ...])` - Generate polynomial and interaction features.\n",
    "\n",
    "**These can be used just like the scikit-learn versions, except that:**\n",
    "\n",
    "* They operate on dask collections in parallel\n",
    "\n",
    "* `.transform` will return a `dask.array` or `dask.dataframe` when the input is a dask collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5ea20",
   "metadata": {},
   "source": [
    "### Create Scikit-Learn Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a3cce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17a5fa",
   "metadata": {},
   "source": [
    "**Because of some issue with scikit-learn I could not excute following lines, please folow the presentation**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efa5242f",
   "metadata": {},
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e3f5000",
   "metadata": {},
   "source": [
    "X, y = make_classification(n_samples=1000, random_state=0)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ee97603",
   "metadata": {},
   "source": [
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "              \"kernel\": ['rbf', 'poly', 'sigmoid'],\n",
    "              \"shrinking\": [True, False]}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(gamma='auto', random_state=0, probability=True),\n",
    "                           param_grid=param_grid,\n",
    "                           return_train_score=False,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "216e2878",
   "metadata": {},
   "source": [
    "km = dask_ml.cluster.KMeans(n_clusters=3, init_max_iter=2, oversampling_factor=10)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cccad57",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[::10000, 0], X[::10000, 1], marker='.', c=km.labels_[::10000],\n",
    "           cmap='viridis', alpha=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33e9ac",
   "metadata": {},
   "source": [
    "### Training on Large Datasets\n",
    "\n",
    "Most estimators in scikit-learn are designed to work on in-memory arrays. Training with larger datasets may require different algorithms.\n",
    "\n",
    "All of the algorithms implemented in Dask-ML work well on larger than memory datasets, which you might store in a dask array or dataframe."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f725cab5",
   "metadata": {},
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c827cf7e",
   "metadata": {},
   "source": [
    "import dask_ml.datasets\n",
    "import dask_ml.cluster\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd217f70",
   "metadata": {},
   "source": [
    "In this example, dask_ml.datasets.make_blobs  generates some random dask arrays\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0831a83",
   "metadata": {},
   "source": [
    "X, y = dask_ml.datasets.make_blobs(n_samples=10000000,\n",
    "                                   chunks=1000000,\n",
    "                                   random_state=0,\n",
    "                                   centers=3)\n",
    "X = X.persist()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d85c4",
   "metadata": {},
   "source": [
    "Here, we use the k-means implemented in Dask-ML to cluster the points. It uses the k-means|| (read: “k-means parallel”) initialization algorithm, which scales better than k-means++. All of the computation, both during and after initialization, can be done in parallel."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8769ea69",
   "metadata": {},
   "source": [
    "km = dask_ml.cluster.KMeans(n_clusters=3, init_max_iter=2, oversampling_factor=10)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "167b0aa7",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[::10000, 0], X[::10000, 1], marker='.', c=km.labels_[::10000],\n",
    "           cmap='viridis', alpha=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2173b3",
   "metadata": {},
   "source": [
    "### Dask ML- Scikit-Learn & Joblib\n",
    "As an example you might distribute a randomized cross validated parameter search as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1efb4dbc",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "\n",
    "import joblib\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "client = Client(processes=False)             # create local cluster\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "param_space = {\n",
    "    'C': np.logspace(-6, 6, 13),\n",
    "    'gamma': np.logspace(-8, 8, 17),\n",
    "    'tol': np.logspace(-4, -1, 4),\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "model = SVC(kernel='rbf')\n",
    "search = RandomizedSearchCV(model, param_space, cv=3, n_iter=50, verbose=10)\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    search.fit(digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85605c37",
   "metadata": {},
   "source": [
    "### Dask ML - XGBoost & LightGBM\n",
    "\n",
    "XGBoost is a powerful and popular library for gradient boosted trees. For larger datasets or faster training XGBoost also provides a distributed computing solution. LightGBM is another library similar to XGBoost; it also natively supplies native distributed training for decision trees.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b57da0b",
   "metadata": {},
   "source": [
    "from dask.distributed import Client\n",
    "client = Client('scheduler-address:8786')\n",
    "\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_parquet('s3://...')\n",
    "\n",
    "# Split into training and testing data\n",
    "train, test = df.random_split([0.8, 0.2])\n",
    "\n",
    "# Separate labels from data\n",
    "train_labels = train.x > 0\n",
    "test_labels = test.x > 0\n",
    "\n",
    "del train['x']  # remove informative column from data\n",
    "del test['x']  # remove informative column from data\n",
    "\n",
    "# from xgboost import XGBRegressor  # change import\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "\n",
    "est = XGBRegressor(...)\n",
    "est.fit(train, train_labels)\n",
    "\n",
    "prediction = est.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f30f7",
   "metadata": {},
   "source": [
    "Dask sets up XGBoost’s master process on the Dask scheduler and XGBoost’s worker processes on Dask’s worker processes. Then it moves all of the Dask dataframes’ constituent Pandas dataframes to XGBoost and lets XGBoost train. Fortunately, because XGBoost has an excellent Python interface, all of this can happen in the same process without any data transfer. The two distributed services can operate together on the same data.\n",
    "\n",
    "When XGBoost is finished training Dask cleans up the XGBoost infrastructure and continues on as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496de92",
   "metadata": {},
   "source": [
    "More information about Dask ML you can find by following links:\n",
    "\n",
    "* https://ml.dask.org/\n",
    "* https://github.com/Svetave/dask-tutorial/blob/main/08_machine_learning.ipynb\n",
    "* https://examples.dask.org/machine-learning.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa48783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
